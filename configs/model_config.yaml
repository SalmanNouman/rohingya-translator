model:
  name: "facebook/mbart-large-50"  # Base model name
  max_length: 128
  src_lang: "en_XX"
  tgt_lang: "ar_AR"  # Using Arabic as a proxy for Rohingya
  special_tokens:
    - "<roh>"

training:
  num_train_epochs: 10
  per_device_train_batch_size: 1  # Start with small batch size
  per_device_eval_batch_size: 1
  warmup_steps: 500
  weight_decay: 0.01
  logging_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3  # Keep last 3 checkpoints for safety
  fp16: true  # Use mixed precision training
  gradient_accumulation_steps: 16  # Accumulate gradients to simulate larger batch
  gradient_checkpointing: true  # Enable gradient checkpointing
  max_grad_norm: 1.0  # Clip gradients
  dataloader_num_workers: 0  # Safer for cloud environment
  torch_compile: false  # Disable torch compilation
  learning_rate: 2e-5
  optim: "adamw_torch"  # Use standard AdamW instead of fused
  resume_from_checkpoint: null  # Set to checkpoint path to resume training
  
  # Simplified DeepSpeed config for better stability
  deepspeed:
    zero_optimization:
      stage: 1  # Use stage 1 instead of 2 for better stability
      offload_optimizer: false  # Keep optimizer on GPU
      memory_efficient_linear: true
    activation_checkpointing:
      partition_activations: true
    aio:
      block_size: 1048576
      queue_depth: 8
      single_submit: false
      overlap_events: true
    
  max_split_size_mb: 128  # Prevent memory fragmentation
  ddp_find_unused_parameters: false
  
  # New: Backup saving options
  save_safeguards:
    verify_save: true  # Verify saved files
    local_backup: true  # Keep local backup until GCS upload confirmed
    save_on_cpu: true  # Move to CPU before saving if CUDA fails

data:
  train_file: "gs://rohingya-translator-vertex/data/processed/train"
  valid_file: "gs://rohingya-translator-vertex/data/processed/val"
  test_file: "gs://rohingya-translator-vertex/data/processed/test"
  max_length: 128
  
tokenizer:
  model_name: "facebook/mbart-large-50"
  special_tokens:
    - "<roh>"  # Rohingya special token
  truncation: true
  padding: "max_length"
  return_tensors: "pt"
