model:
  base_model_name: "facebook/nllb-200-distilled-600M"
  max_length: 128
  src_lang: "eng_Latn"
  tgt_lang: "ben_Beng"  # Proxy
  special_tokens:
    - "<roh>"

training:
  num_train_epochs: 10
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  warmup_steps: 500
  weight_decay: 0.01
  logging_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 1000
  save_steps: 1000
  save_total_limit: 3
  fp16: true
  gradient_accumulation_steps: 16
  gradient_checkpointing: true
  max_grad_norm: 1.0
  dataloader_num_workers: 0
  torch_compile: false
  learning_rate: 2e-5
  optim: "adamw_torch"
  resume_from_checkpoint: null
  
  max_split_size_mb: 128
  ddp_find_unused_parameters: false
  
  save_safeguards:
    verify_save: true
    local_backup: true
    save_on_cpu: true

data:
  train_file: "data/processed/train"
  valid_file: "data/processed/val"
  test_file: "data/processed/test"
  max_length: 128
  
tokenizer:
  model_name: "facebook/nllb-200-distilled-600M"
  special_tokens:
    - "<roh>"
  truncation: true
  padding: "max_length"
  return_tensors: "pt"
